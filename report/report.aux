\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand \oddpage@label [2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{1606.01540}
\citation{journals/corr/LillicrapHPHETS15}
\@writefile{toc}{\contentsline {section}{\numberline {1}Definition of the problem}{3}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Project Overview}{3}{subsection.1.1}}
\@writefile{toc}{\contentsline {paragraph}{}{3}{section*.1}}
\@writefile{toc}{\contentsline {paragraph}{}{3}{section*.2}}
\@writefile{toc}{\contentsline {paragraph}{}{3}{section*.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Problem Statement}{3}{subsection.1.2}}
\@writefile{toc}{\contentsline {paragraph}{}{3}{section*.4}}
\@writefile{toc}{\contentsline {paragraph}{Actor Critic algorithm}{4}{section*.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Metrics}{4}{subsection.1.3}}
\@writefile{toc}{\contentsline {paragraph}{}{4}{section*.6}}
\@writefile{toc}{\contentsline {paragraph}{}{4}{section*.7}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Analysis}{5}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Data Exploration: RoboschoolHumanoid-v1}{5}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Observation space}{5}{subsubsection.2.1.1}}
\newlabel{subsubsec:obs_space}{{2.1.1}{5}{Observation space}{subsubsection.2.1.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{5}{section*.8}}
\@writefile{toc}{\contentsline {paragraph}{}{5}{section*.9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Action space}{5}{subsubsection.2.1.2}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{5}{section*.10}}
\@writefile{toc}{\contentsline {paragraph}{}{5}{section*.11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Reward}{6}{subsubsection.2.1.3}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{6}{section*.12}}
\@writefile{toc}{\contentsline {paragraph}{}{6}{section*.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Roboschool environment\relax }}{7}{figure.caption.14}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:exploratory}{{1}{7}{Roboschool environment\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Exploratory Visualization}{7}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Algorithm and Techniques}{7}{subsection.2.3}}
\@writefile{toc}{\contentsline {paragraph}{Deep Deterministic Policy Gradient}{7}{section*.15}}
\citation{journals/corr/LillicrapHPHETS15}
\citation{journals/corr/LillicrapHPHETS15}
\citation{journals/corr/LillicrapHPHETS15}
\citation{GuLilGhaTurLev17}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces First, the environment gives the first state to the actor and it chose the action following the policy. The action is given back to the environment and a reward is computed, meaning how good the action was for that state, and sent to the critic network. The critic computes the $Q$ value of that action by minimizing the loss and gives the error to the actor in order to let him train on it.\relax }}{8}{figure.caption.16}}
\newlabel{fig:actor-critic}{{2}{8}{First, the environment gives the first state to the actor and it chose the action following the policy. The action is given back to the environment and a reward is computed, meaning how good the action was for that state, and sent to the critic network. The critic computes the $Q$ value of that action by minimizing the loss and gives the error to the actor in order to let him train on it.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {paragraph}{}{8}{section*.17}}
\@writefile{toc}{\contentsline {paragraph}{}{8}{section*.18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Benchmark}{8}{subsection.2.4}}
\@writefile{toc}{\contentsline {paragraph}{}{8}{section*.21}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Algorithm from \citeauthor  {journals/corr/LillicrapHPHETS15} paper\relax }}{9}{figure.caption.19}}
\newlabel{fig:algoDDPG}{{3}{9}{Algorithm from \citeauthor {journals/corr/LillicrapHPHETS15} paper\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{9}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Data Preprocessing}{9}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Random Policy\relax }}{10}{figure.caption.20}}
\newlabel{fig:randompolicy}{{4}{10}{Random Policy\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Average return over episodes in HalfCheetah-v1 and Humanoid-v1 during learning, comparing Q-Prop against other model-free algorithms. Q-Prop with vanilla policy gradient outperforms TRPO on HalfCheetah. Q-Prop significantly outperforms TRPO in convergence time on Humanoid.\relax }}{10}{figure.caption.22}}
\newlabel{fig:benchmark}{{5}{10}{Average return over episodes in HalfCheetah-v1 and Humanoid-v1 during learning, comparing Q-Prop against other model-free algorithms. Q-Prop with vanilla policy gradient outperforms TRPO on HalfCheetah. Q-Prop significantly outperforms TRPO in convergence time on Humanoid.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Implementation}{10}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Project Structure}{10}{subsubsection.3.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Replay buffer}{11}{subsubsection.3.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Neural networks: Actor and Critic}{11}{subsubsection.3.2.3}}
\@writefile{toc}{\contentsline {paragraph}{Keras vs Tensorflow}{11}{section*.23}}
\@writefile{toc}{\contentsline {paragraph}{The Actor}{11}{section*.24}}
\@writefile{toc}{\contentsline {paragraph}{The Critic}{12}{section*.25}}
\@writefile{toc}{\contentsline {paragraph}{Target networks}{12}{section*.26}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Training}{12}{subsubsection.3.2.4}}
\@writefile{toc}{\contentsline {paragraph}{The Actor}{12}{section*.27}}
\@writefile{toc}{\contentsline {paragraph}{The Critic}{12}{section*.28}}
\@writefile{toc}{\contentsline {paragraph}{Process}{13}{section*.29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.5}Noise}{14}{subsubsection.3.2.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.6}The Agent}{14}{subsubsection.3.2.6}}
\citation{journals/corr/LillicrapHPHETS15}
\citation{DBLP:journals/corr/abs-1801-10425}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Refinement}{15}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Network definition}{15}{subsubsection.3.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Hyper Parameters}{15}{subsubsection.3.3.2}}
\newlabel{subsubsec:hp_params}{{3.3.2}{15}{Hyper Parameters}{subsubsection.3.3.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Hyper parameters description and definition for training purpose\relax }}{16}{table.caption.30}}
\newlabel{tab:hyperparams}{{1}{16}{Hyper parameters description and definition for training purpose\relax }{table.caption.30}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{16}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model Evaluation and Validation}{16}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Implementation}{16}{subsubsection.4.1.1}}
\@writefile{toc}{\contentsline {paragraph}{}{16}{section*.31}}
\@writefile{toc}{\contentsline {paragraph}{}{16}{section*.33}}
\@writefile{toc}{\contentsline {paragraph}{}{16}{section*.34}}
\citation{journals/corr/LillicrapHPHETS15}
\newlabel{fig:criticlossmountain}{{6a}{17}{Critic loss\relax }{figure.caption.32}{}}
\newlabel{sub@fig:criticlossmountain}{{a}{17}{Critic loss\relax }{figure.caption.32}{}}
\newlabel{fig:avgrewardmountain}{{6b}{17}{Average reward\relax }{figure.caption.32}{}}
\newlabel{sub@fig:avgrewardmountain}{{b}{17}{Average reward\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Training of MountainCarContinuous environments\relax }}{17}{figure.caption.32}}
\newlabel{fig:mountaincartrain}{{6}{17}{Training of MountainCarContinuous environments\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Tweaking the model}{17}{subsubsection.4.1.2}}
\@writefile{toc}{\contentsline {paragraph}{}{17}{section*.38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Evaluation}{17}{subsubsection.4.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Justification}{17}{subsection.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{17}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Free-Form Visualization}{17}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Reflection}{17}{subsection.5.2}}
\@writefile{toc}{\contentsline {paragraph}{}{17}{section*.41}}
\citation{GuLilGhaTurLev17}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Soft target update operations graph\relax }}{18}{figure.caption.35}}
\newlabel{fig:softtargetgraph}{{7}{18}{Soft target update operations graph\relax }{figure.caption.35}{}}
\@writefile{toc}{\contentsline {paragraph}{}{18}{section*.42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Improvement}{18}{subsection.5.3}}
\bibdata{report}
\bibcite{1606.01540}{{1}{2016}{{Brockman et~al.}}{{Brockman, Cheung, Pettersson, Schneider, Schulman, Tang, and Zaremba}}}
\bibcite{GuLilGhaTurLev17}{{2}{2017}{{Gu et~al.}}{{Gu, Lillicrap, Ghahramani, Turner, and Levine}}}
\bibcite{journals/corr/LillicrapHPHETS15}{{3}{2015}{{Lillicrap et~al.}}{{Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa, Silver, and Wierstra}}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Critic network graph\relax }}{19}{figure.caption.36}}
\newlabel{fig:criticgraph}{{8}{19}{Critic network graph\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Actor network graph\relax }}{19}{figure.caption.37}}
\newlabel{fig:actorgraph}{{9}{19}{Actor network graph\relax }{figure.caption.37}{}}
\bibstyle{plainnat}
\newlabel{fig:bn_avgreward}{{10a}{20}{\relax }{figure.caption.39}{}}
\newlabel{sub@fig:bn_avgreward}{{a}{20}{\relax }{figure.caption.39}{}}
\newlabel{fig:bn_loss}{{10b}{20}{\relax }{figure.caption.39}{}}
\newlabel{sub@fig:bn_loss}{{b}{20}{\relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Comparison of learning with and without batch normalization\relax }}{20}{figure.caption.39}}
\newlabel{fig:bn_tweakinggraph}{{10}{20}{Comparison of learning with and without batch normalization\relax }{figure.caption.39}{}}
\newlabel{fig:bn_avgreward}{{11a}{20}{\relax }{figure.caption.40}{}}
\newlabel{sub@fig:bn_avgreward}{{a}{20}{\relax }{figure.caption.40}{}}
\newlabel{fig:bn_loss}{{11b}{20}{\relax }{figure.caption.40}{}}
\newlabel{sub@fig:bn_loss}{{b}{20}{\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Comparison of learning with and without dropout\relax }}{20}{figure.caption.40}}
\newlabel{fig:bn_tweakinggraph}{{11}{20}{Comparison of learning with and without dropout\relax }{figure.caption.40}{}}
